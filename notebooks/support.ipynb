{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18618dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary librarise\n",
    "import panda as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "pd.set_option('display.max_colums', None)\n",
    "pd.set_option('disply.max_rows', None)\n",
    "\n",
    "#read cvs file\n",
    "df = pd.read_csv('C:\\\\Lincolnuni\\\\COMP_647\\\\project_1\\\\comp647\\\\data_kaggle\\\\crop_yield_dataset.csv')\n",
    "\n",
    "# Display the first 5 rows\n",
    "df.head()\n",
    "\n",
    "# Returns the number of rows and columns in the DataFrame.\n",
    "df.shape\n",
    "\n",
    "# Returns information about the DataFarme, including data tyes and non-null count\n",
    "df.info()\n",
    "\n",
    "# Returns summary statistics for numberical columns in the DataFrame.\n",
    "df.describe().transpose()\n",
    "\n",
    "### Handle Duplicates\n",
    "df.colums\n",
    "\n",
    "# Check for duplicates in each column and print the count of duplicates for each column\n",
    "for col in df.colnums:\n",
    "    duplicated_count = df[col].duplicated().sum()\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"Duplicate Count: {duplicated_count}\")\n",
    "    print(\"*\"*50)\n",
    "\n",
    "# Duplicate records by multiple columns\n",
    "duplicated_multi_cols = df[df.duplicated(\n",
    "    subset=['ADDRESS', 'FLOOR_AREA'],\n",
    "    Keep=False\n",
    "    )]\n",
    "duplicated_multi_cols.shape\n",
    "\n",
    "duplicated_multi_cols.sort_values('ADDRESS').head(10)\n",
    "\n",
    "### Handle Lrrelavant Data\n",
    "\n",
    "# Columns where all values are the same (constant features)\n",
    "\n",
    "constant_features = [col for col in df.column if df[col].nunique()==1]\n",
    "print(\"Contant features:\", constant_features)\n",
    "\n",
    "\n",
    "#Remove constant features form the DataFrame\n",
    "df_no_constant_features = df.drop(column=constant_features)\n",
    "\n",
    "\n",
    "# Columns with mostly missing values (e.g., more than x% missing)\n",
    "threshold = 30\n",
    "print(f\"Total records {df.shape[0]}\")\n",
    "print(\"*\"*50)\n",
    "for col in df.columns:\n",
    "    missing_count = df[col].isnull().sum()\n",
    "    missing_ratio = (missing_count / df.shape[0]) * 100\n",
    "    if missing_ratio > threshold:\n",
    "        print(f\"Column:{col} has {missing_count} missing values ({missing_ratio:/2f}%)\")        \n",
    "        print(\"*\"*50)\n",
    "        \n",
    "# Remove columns with more than x% missing values.\n",
    "colums_to_drop = [col for col in df.columns if (df[col].isnull().sum()/df.share[0])*100 > threshold]        \n",
    "df_low_missing_data = df.drop(columns = colums_to_drop)\n",
    "\n",
    "\n",
    "### Handle Missing Values\n",
    "\n",
    "# Disply tge DataFrame having missing data.\n",
    "df_missing_data = df[df.isnull().any(axis=1)]\n",
    "df_missing_data.shape\n",
    "df_missing_data.tall()\n",
    "\n",
    "# Identify numberical and categorical columns\n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "print(\"Numberical colums:\", numerical_columns)\n",
    "print(\"Categorical columns:\", categorical_columns)\n",
    "\n",
    "# Get the list of columns with missing values only for numerical columns.\n",
    "missing_numerical_columns = df[numerical_columns].isnull().any()\n",
    "missing_numerical_columns = missing_numerical_columns[missing_numerical_columns].index\n",
    "print(\"Numerical columns with missing values:\", missing_numerical_columns.tolist())\n",
    "\n",
    "\n",
    "# Get the list of columns with missing values only for categorical columns.\n",
    "missing_categorical_columns = df[categorical_columns].isnull().any()\n",
    "missing_categorical_columns = missing_categorical_columns[missing_categorical_columns].index\n",
    "print(\"categorical columns with missing values:\", missing_categorical_columns.tolist())\n",
    "                         \n",
    "# Remove all rows with missing values in the DeataFrame\n",
    "dr_no_missing_data = df.dropna()\n",
    "\n",
    "\n",
    "# Remove all columns with missing values in the DeataFrame\n",
    "dr_no_missing_columns = df.dropna(axis=1)\n",
    "\n",
    "# Fill missing values with a specific value (e.g., 0 for numerical columns, 'Unknown' for categorical columns)\n",
    "df_filled = df.copy()\n",
    "df_filled[numerical_columns] = df_filled[numerical_columns].fillna(0.9999)\n",
    "for col in categorical_columns:\n",
    "    df_filled[col] = df_filled[col].fillna('Unknown')                         \n",
    "                                    \n",
    "\n",
    "# Check the filled data in DataFrame.\n",
    "selected_row = df_filled.iloc[{33553,33654}]\n",
    "selected_row\n",
    "\n",
    "# Fill missing values with the mean, median, or mode of the colum\n",
    "\n",
    "# Fill with mean ( numeric columns)\n",
    "df_filled_mean = df.fillna(df.mean(numerical_only=True))\n",
    "\n",
    "# Fill with median ( numeric columns)\n",
    "df_filled_median = df.fillna(df.median(numcal_only=True))\n",
    "\n",
    "# Fill with mode (all colums) .iloc[0] is used to get the first mode if there are multiple modes for a given column\n",
    "df_filled_mode = df.fillna(df.mode().iloc[0])\n",
    "\n",
    "# check the filled values by mean. --> BUILD_YEAR, and NEAREST_SCH_RANK should be filled with mean.\n",
    "df_filled_raws = df_filled_mean.iloc[{33653,33564}]\n",
    "df_filled_raws\n",
    "\n",
    "### Handle Outliers\n",
    "# Grab outliers because outliers are the extreme value or significantly\n",
    "# different from other values in the dataset. In regression analysis.\n",
    "# outliers can lead to inaccurate and unreliable prediction resultes\n",
    "# Therefore, handing outliers is nessary in regression analysis\n",
    " \n",
    "# IQR(Interquartile Range): Remove points outside Q1 - 1.5*IQR or Q3 + 1.5*IQR\n",
    "# \n",
    "\n",
    "def find_outliers_IQR_menthod(input_df, variabvle):\n",
    "    IQR = input_df[variabvle].quantile(0.75) - input_df[variabvle].quantile(0.25)\n",
    "\n",
    "    lower_limit = input_df[variabvle].quantile(0.25) - (IQR*1.5)\n",
    "    upper_limit = input_df[variabvle].quantile(0.75) + (IQR*1.5)\n",
    "\n",
    "    return lower_limit, upper_limit\n",
    "\n",
    "# Find lower and upper limit form targer\n",
    "feature = 'PRICE'\n",
    "lower, upper = find_outliers_IQR_menthod(df, feature)\n",
    "lower, upper\n",
    "\n",
    "#REmove outliers w.r.t. the feature\n",
    "df_cleaned = df[(df[feature] > lower) &(df[feature] < upper)]\n",
    "\n",
    "print(f'Cleaned dataset : {df_cleaned.shape}')\n",
    "print(f'Outliers count : {len(df)-len(df_cleaned)}')\n",
    "\n",
    "\n",
    "# Probability plots befire anbd after handing outliers\n",
    "# A probability plot (probplot)-typically used in normality testing, is also a helpful visual tool for identifying oiytliers\n",
    "# and assessing distribution fit.\n",
    "\n",
    "# Points far from the line        Possible outliers\n",
    "# Points far at the ends only     Outliers in tails\n",
    "# Sudden jumps in spacing         Data irregulartities or outliers\n",
    "# S-shape curve                   Non-normality + possible outliers\n",
    "\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "stats.probplot(df[feature], plot=plt)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "stats.probplot(df_cleaned[feature], plot=plt)\n",
    "\n",
    "\n",
    "\n",
    "def find_outliers_ZScore_method(input_df, variable):\n",
    "    \n",
    "   df_z_scores = input_df.copy()\n",
    "\n",
    "   # Calculate Z-scores for the specified variable droping any rows having NaN values\n",
    "   z_scores = np.abs(stats.zscore(input_df[variable].dropna())) \n",
    "   \n",
    "   # Add Z-scores as a new column\n",
    "   df_z_scores[variable + '_Z'] = z_scores\n",
    "   return df_z_scores \n",
    "\n",
    "# Calculate Z-scores for the specified feature\n",
    "\n",
    "df_z_scores = find_outliers_ZScore_method(df.copy(),feature)\n",
    "df_z_scores.head()\n",
    "\n",
    "\n",
    "# Remove outliers w.r.t. the Feature. Remove data points where |Z| > 3.\n",
    "\n",
    "df_z_scores_cleaned = df_z_scores[df_z_scores[feature+' _Z'] < 3]\n",
    "\n",
    "print(f'cleaned dataset : {df_z_scores_cleaned.shape}')\n",
    "print(f'Outliers count : {len(df_z_scores)-len(df_cleaned)}')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.subplot(1,2,3)\n",
    "stats.probplot(df_z_scores[feature], plot=plt)\n",
    "\n",
    "\n",
    "### Pandas Profile Report\n",
    "\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df, title=\"Profiling Report\")\n",
    "profile.to_file(\"ProfilingReport.html\")\n",
    "profile.to_file(\"ProfilingReport.json\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                              \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f414761a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
